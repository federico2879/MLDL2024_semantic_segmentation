{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8476013,"sourceType":"datasetVersion","datasetId":5006503},{"sourceId":8578946,"sourceType":"datasetVersion","datasetId":5130409},{"sourceId":8943499,"sourceType":"datasetVersion","datasetId":5381532}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/federico2879/MLDL2024_semantic_segmentation.git","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:14.317288Z","iopub.execute_input":"2024-07-13T08:25:14.317557Z","iopub.status.idle":"2024-07-13T08:25:15.927777Z","shell.execute_reply.started":"2024-07-13T08:25:14.317532Z","shell.execute_reply":"2024-07-13T08:25:15.926438Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'MLDL2024_semantic_segmentation'...\nremote: Enumerating objects: 699, done.\u001b[K\nremote: Counting objects: 100% (350/350), done.\u001b[K\nremote: Compressing objects: 100% (172/172), done.\u001b[K\nremote: Total 699 (delta 207), reused 295 (delta 174), pack-reused 349\u001b[K\nReceiving objects: 100% (699/699), 333.54 KiB | 3.47 MiB/s, done.\nResolving deltas: 100% (414/414), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom MLDL2024_semantic_segmentation.datasets.cityscapes import CityScapes\nfrom MLDL2024_semantic_segmentation.datasets.gta5 import GTA5\nfrom MLDL2024_semantic_segmentation.models.bisenet.build_bisenet import *\nfrom MLDL2024_semantic_segmentation.train import *\nfrom MLDL2024_semantic_segmentation.train_adv import * \nfrom MLDL2024_semantic_segmentation.utils import *\nfrom MLDL2024_semantic_segmentation.models.IOU import * \nfrom MLDL2024_semantic_segmentation.models.Adversarial.discriminator import FCDiscriminator","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:15.930852Z","iopub.execute_input":"2024-07-13T08:25:15.931335Z","iopub.status.idle":"2024-07-13T08:25:20.985246Z","shell.execute_reply.started":"2024-07-13T08:25:15.931287Z","shell.execute_reply":"2024-07-13T08:25:20.984440Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:20.989586Z","iopub.execute_input":"2024-07-13T08:25:20.989852Z","iopub.status.idle":"2024-07-13T08:25:21.039904Z","shell.execute_reply.started":"2024-07-13T08:25:20.989828Z","shell.execute_reply":"2024-07-13T08:25:21.038773Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"# Setup fixed parameters\nnum_epochs = 50\nnum_classes = 19\nbatch_size = 4\n\nGTA_dim = (1280, 720)\nCityScapes_dim = (1024, 512)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:21.042279Z","iopub.execute_input":"2024-07-13T08:25:21.042627Z","iopub.status.idle":"2024-07-13T08:25:21.052652Z","shell.execute_reply.started":"2024-07-13T08:25:21.042602Z","shell.execute_reply":"2024-07-13T08:25:21.051819Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Transformations\ntransform_image = {\n    'GTA': transforms.Compose([\n        transforms.Resize(GTA_dim),\n        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),\n        transforms.RandomHorizontalFlip(0.15),\n        transforms.GaussianBlur(kernel_size=3, sigma=(0.2, 0.8)),\n        transforms.ToTensor(),        \n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ]),\n    'CityScapes': transforms.Compose([\n        transforms.Resize(CityScapes_dim),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n}\n                   \ntransform_target = {\n    'GTA': transforms.Compose([\n        transforms.Resize(GTA_dim, interpolation=InterpolationMode.NEAREST),\n    ]),\n    'CityScapes': transforms.Compose([\n        transforms.Resize(CityScapes_dim, interpolation=InterpolationMode.NEAREST)\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:21.053564Z","iopub.execute_input":"2024-07-13T08:25:21.053827Z","iopub.status.idle":"2024-07-13T08:25:21.062751Z","shell.execute_reply.started":"2024-07-13T08:25:21.053803Z","shell.execute_reply":"2024-07-13T08:25:21.061787Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create dataloaders\n\ndataset_source = GTA5('/kaggle/input/mldl-gta5/GTA', \n                     transform = transform_image['GTA'], \n                     label_transform = transform_target['GTA'])\nsource_dataloader = DataLoader(dataset_source, batch_size=batch_size, shuffle=True)\n\ndataset_target = CityScapes('/kaggle/input/cityscapes/Cityscapes/Cityspaces', \n                          split = 'train', transform = transform_image['CityScapes'], \n                          label_transform = transform_target['CityScapes'])\ntarget_dataloader = DataLoader(dataset_target, batch_size=batch_size, shuffle=True)\n\ndataset_target_test = CityScapes('/kaggle/input/cityscapes/Cityscapes/Cityspaces', \n                          split = 'val', transform = transform_image['CityScapes'], \n                          label_transform = transform_target['CityScapes'])\ntarget_dataloader_test = DataLoader(dataset_target_test, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:21.063937Z","iopub.execute_input":"2024-07-13T08:25:21.064260Z","iopub.status.idle":"2024-07-13T08:25:21.516478Z","shell.execute_reply.started":"2024-07-13T08:25:21.064231Z","shell.execute_reply":"2024-07-13T08:25:21.515635Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FCDiscriminator(nn.Module):\n\n    def __init__(self, num_classes, ndf = 64):\n        super(FCDiscriminator, self).__init__()\n\n        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n        self.sigmoid = nn.Sigmoid()\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.leaky_relu(x)\n        x = self.conv2(x)\n        x = self.leaky_relu(x)\n        x = self.conv3(x)\n        x = self.leaky_relu(x)\n        x = self.conv4(x)\n        x = self.leaky_relu(x)\n        x = self.classifier(x)\n        x = self.up_sample(x)\n        x = self.sigmoid(x) \n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:21.517559Z","iopub.execute_input":"2024-07-13T08:25:21.517848Z","iopub.status.idle":"2024-07-13T08:25:21.528670Z","shell.execute_reply.started":"2024-07-13T08:25:21.517823Z","shell.execute_reply":"2024-07-13T08:25:21.527613Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef train_adv(model, discr, seg_loss, bce_loss, targetloader, sourceloader, optimizer, opt_discr, \n              device, num_classes):\n    model.train()\n    discr.train()\n  \n    # Labels for adversarial training\n    source_label = 0\n    target_label = 1\n    \n    # Create iterators\n    sourceloader_iter = iter(sourceloader)\n    targetloader_iter = iter(targetloader)\n    \n    max_iterations = min(len(targetloader), len(sourceloader))\n    \n    for idx in range(max_iterations):\n\n        optimizer.zero_grad()\n        opt_discr.zero_grad()\n\n        ##### Train G #####\n\n        # Don't accumulate grads in D\n        for param in discr.parameters():\n            param.requires_grad = False\n\n        # Train with source\n        batch = next(sourceloader_iter)\n        images, labels = batch\n        images = images.to(device)\n        labels = labels.squeeze(dim=1).long().to(device)\n\n        pred = model(images)[0]\n        \n        loss_seg = seg_loss(pred, labels)\n        loss_seg.backward()\n\n        # Train with target\n        batch = next(targetloader_iter)\n        images, _ = batch\n        images = images.to(device)\n\n        pred_target = model(images)[0]\n\n        D_out = discr(F.softmax(pred_target, dim=1))\n\n        loss_adv_target = bce_loss(D_out, torch.FloatTensor(D_out.data.size()).fill_(source_label).to(device))\n        loss_adv_target.backward()\n\n        ##### Train D #####\n\n        # Bring back requires_grad\n        for param in discr.parameters():\n            param.requires_grad = True\n\n        # Train with source\n        pred = pred.detach()\n\n        D_out = discr(F.softmax(pred, dim=1))\n\n        loss_D = bce_loss(D_out, torch.FloatTensor(D_out.data.size()).fill_(source_label).to(device))\n        loss_D.backward()\n\n        # Train with target\n        pred_target = pred_target.detach()\n        \n        D_out = discr(F.softmax(pred_target, dim=1))\n\n        loss_D = bce_loss(D_out, torch.FloatTensor(D_out.data.size()).fill_(target_label).to(device))\n        loss_D.backward()\n\n        optimizer.step()\n        opt_discr.step()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:21.530123Z","iopub.execute_input":"2024-07-13T08:25:21.530810Z","iopub.status.idle":"2024-07-13T08:25:21.545241Z","shell.execute_reply.started":"2024-07-13T08:25:21.530776Z","shell.execute_reply":"2024-07-13T08:25:21.544256Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Inizialization of the models\ngenerator = BiSeNet(num_classes=num_classes, context_path=\"resnet18\").to(device)\ndiscriminator = FCDiscriminator(num_classes=num_classes).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:21.546639Z","iopub.execute_input":"2024-07-13T08:25:21.547080Z","iopub.status.idle":"2024-07-13T08:25:24.754444Z","shell.execute_reply.started":"2024-07-13T08:25:21.547047Z","shell.execute_reply":"2024-07-13T08:25:24.753438Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 143MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:01<00:00, 153MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Optimizers\noptimizer_G = torch.optim.SGD(generator.parameters(), lr=2.5e-2, momentum=0.9, weight_decay=1e-4)\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.9, 0.99))\n\n# Loss functions\nsegmentation_loss_fn = nn.CrossEntropyLoss(ignore_index=255)\nadversarial_loss_fn = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:24.755607Z","iopub.execute_input":"2024-07-13T08:25:24.755894Z","iopub.status.idle":"2024-07-13T08:25:24.762530Z","shell.execute_reply.started":"2024-07-13T08:25:24.755868Z","shell.execute_reply":"2024-07-13T08:25:24.761555Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# metrics\nmeanIOU = np.zeros((num_epochs,1))\nIOU = np.zeros((num_epochs,num_classes))\nloss = np.zeros((num_epochs,1))","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:24.763625Z","iopub.execute_input":"2024-07-13T08:25:24.763948Z","iopub.status.idle":"2024-07-13T08:25:24.771160Z","shell.execute_reply.started":"2024-07-13T08:25:24.763918Z","shell.execute_reply":"2024-07-13T08:25:24.770245Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:34.231469Z","iopub.execute_input":"2024-07-13T08:25:34.232114Z","iopub.status.idle":"2024-07-13T08:25:34.236470Z","shell.execute_reply.started":"2024-07-13T08:25:34.232079Z","shell.execute_reply":"2024-07-13T08:25:34.235542Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    poly_lr_scheduler(optimizer_G, 2.5e-2, epoch, lr_decay_iter=1, max_iter=num_epochs, power=0.9)\n    poly_lr_scheduler(optimizer_D, 1e-4, epoch, lr_decay_iter=1, max_iter=num_epochs, power=0.9)\n    \n    train_adv(generator, discriminator, segmentation_loss_fn, adversarial_loss_fn, target_dataloader, \n              source_dataloader, optimizer_G, optimizer_D, device, num_classes)\n    \n    meanIOU[epoch], IOU[epoch,:], loss[epoch] = test(generator, target_dataloader_test, segmentation_loss_fn, num_classes)\n    print(f\"epoch: {epoch + 1}, Validation IOU: {meanIOU[epoch,0]:.2f}\")\n    \n    torch.save({\n    'epoch': epoch + 1,\n    'state_dict_gen': generator.state_dict(),\n    'state_dict_dis': discriminator.state_dict(),\n    'optimizer_gen': optimizer_G.state_dict(),\n    'optimizer_dis': optimizer_D.state_dict(),\n    'meanIOU': meanIOU,\n    'IOU': IOU,\n    'loss': loss    \n    },\"checkpoint.pth.tar\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:44:30.004748Z","iopub.execute_input":"2024-07-12T13:44:30.005080Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"epoch: 1, Validation IOU: 0.17\nepoch: 2, Validation IOU: 0.19\nepoch: 3, Validation IOU: 0.19\nepoch: 4, Validation IOU: 0.21\nepoch: 5, Validation IOU: 0.22\nepoch: 6, Validation IOU: 0.24\nepoch: 7, Validation IOU: 0.21\nepoch: 8, Validation IOU: 0.22\nepoch: 9, Validation IOU: 0.21\nepoch: 10, Validation IOU: 0.20\nepoch: 11, Validation IOU: 0.23\nepoch: 12, Validation IOU: 0.20\nepoch: 13, Validation IOU: 0.22\nepoch: 14, Validation IOU: 0.20\nepoch: 15, Validation IOU: 0.22\nepoch: 16, Validation IOU: 0.23\nepoch: 17, Validation IOU: 0.19\nepoch: 18, Validation IOU: 0.24\nepoch: 19, Validation IOU: 0.21\nepoch: 20, Validation IOU: 0.20\nepoch: 21, Validation IOU: 0.22\nepoch: 22, Validation IOU: 0.21\nepoch: 23, Validation IOU: 0.21\nepoch: 24, Validation IOU: 0.23\nepoch: 25, Validation IOU: 0.20\nepoch: 26, Validation IOU: 0.21\nepoch: 27, Validation IOU: 0.24\nepoch: 28, Validation IOU: 0.19\nepoch: 29, Validation IOU: 0.23\nepoch: 30, Validation IOU: 0.23\nepoch: 31, Validation IOU: 0.21\nepoch: 32, Validation IOU: 0.21\nepoch: 33, Validation IOU: 0.23\nepoch: 34, Validation IOU: 0.21\n","output_type":"stream"}]},{"cell_type":"code","source":"from MLDL2024_semantic_segmentation.load_checkpoint import *\n\ngenerator, discriminator, optimizer_G, optimizer_D, start_epoch, meanIOU, IOU, loss \\\n= load_checkpoint_adversarial(generator, discriminator, optimizer_G, optimizer_D, \n                              filename=\"/kaggle/input/adv-s4-p1/checkpoint.pth.tar\")\n\nfor epoch in range(start_epoch,num_epochs):\n    poly_lr_scheduler(optimizer_G, 2.5e-2, epoch, lr_decay_iter=1,\n                      max_iter=num_epochs, power=0.9)\n    poly_lr_scheduler(optimizer_D, 1e-4, epoch, lr_decay_iter=1,\n                      max_iter=num_epochs, power=0.9)\n    train_adv(generator, discriminator, segmentation_loss_fn, adversarial_loss_fn, target_dataloader, \n              source_dataloader, optimizer_G, optimizer_D, device, num_classes)\n    \n    meanIOU[epoch], IOU[epoch,:], loss[epoch] = test(generator, target_dataloader_test, segmentation_loss_fn, num_classes)\n    print(f\"epoch: {epoch + 1}, Validation IOU: {meanIOU[epoch,0]:.2f}\")\n    \n    torch.save({\n    'epoch': epoch + 1,\n    'state_dict_gen': generator.state_dict(),\n    'state_dict_dis': discriminator.state_dict(),\n    'optimizer_gen': optimizer_G.state_dict(),\n    'optimizer_dis': optimizer_D.state_dict(),\n    'meanIOU': meanIOU,\n    'IOU': IOU,\n    'loss': loss    \n    },\"checkpoint.pth.tar\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T08:25:50.312443Z","iopub.execute_input":"2024-07-13T08:25:50.312809Z","iopub.status.idle":"2024-07-13T12:36:43.055126Z","shell.execute_reply.started":"2024-07-13T08:25:50.312779Z","shell.execute_reply":"2024-07-13T12:36:43.054242Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"epoch: 35, Validation IOU: 0.21\nepoch: 36, Validation IOU: 0.21\nepoch: 37, Validation IOU: 0.18\nepoch: 38, Validation IOU: 0.23\nepoch: 39, Validation IOU: 0.20\nepoch: 40, Validation IOU: 0.21\nepoch: 41, Validation IOU: 0.22\nepoch: 42, Validation IOU: 0.23\nepoch: 43, Validation IOU: 0.21\nepoch: 44, Validation IOU: 0.20\nepoch: 45, Validation IOU: 0.23\nepoch: 46, Validation IOU: 0.21\nepoch: 47, Validation IOU: 0.23\nepoch: 48, Validation IOU: 0.23\nepoch: 49, Validation IOU: 0.22\nepoch: 50, Validation IOU: 0.21\n","output_type":"stream"}]},{"cell_type":"code","source":"# final print\n\nprint(f\"Final mIOU: {meanIOU[epoch,0]:.2f}\")\nprint(\"Final IOU classes\")\nprint(IOU[epoch,:])","metadata":{"execution":{"iopub.status.busy":"2024-07-13T12:36:43.056826Z","iopub.execute_input":"2024-07-13T12:36:43.057125Z","iopub.status.idle":"2024-07-13T12:36:43.063052Z","shell.execute_reply.started":"2024-07-13T12:36:43.057100Z","shell.execute_reply":"2024-07-13T12:36:43.062249Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Final mIOU: 0.21\nFinal IOU classes\n[0.75756748 0.10417947 0.69510503 0.16644017 0.15921441 0.04638424\n 0.00729614 0.01063082 0.58255069 0.16430608 0.46191691 0.07882946\n 0.00632881 0.44601786 0.13545487 0.08711545 0.08808932 0.0029727\n 0.00077796]\n","output_type":"stream"}]},{"cell_type":"code","source":"# writing csv\nimport csv\n\nwith open('meanIOU.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(meanIOU)\n\nwith open('IOU.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(IOU)\n\nwith open('loss.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerows(loss)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T12:36:43.064375Z","iopub.execute_input":"2024-07-13T12:36:43.064646Z","iopub.status.idle":"2024-07-13T12:36:43.086710Z","shell.execute_reply.started":"2024-07-13T12:36:43.064622Z","shell.execute_reply":"2024-07-13T12:36:43.086000Z"},"trusted":true},"execution_count":15,"outputs":[]}]}