{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBtXNh76_C51",
        "outputId": "70950714-c3eb-495d-d898-85924aecdf9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLDL2024_semantic_segmentation'...\n",
            "remote: Enumerating objects: 216, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 216 (delta 53), reused 1 (delta 1), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (216/216), 141.56 KiB | 2.78 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n"
          ]
        }
      ],
      "source": [
        "#import os\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "\n",
        "!git clone https://github.com/federico2879/MLDL2024_semantic_segmentation.git\n",
        "\n",
        "from MLDL2024_semantic_segmentation.datasets.importDataset import Download\n",
        "from MLDL2024_semantic_segmentation.datasets.importDataset import Modified_CityScapes\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from MLDL2024_semantic_segmentation.datasets.cityscapes import CityScapes\n",
        "import torchvision.transforms as transforms\n",
        "from MLDL2024_semantic_segmentation.models.deeplabv2.deeplabv2 import *\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FbNG1Dxxb6jr",
        "outputId": "565be201-9866-479a-bb49-04a46f5fda15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore\n",
        "\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def Flops(model, height, width):\n",
        "  image = torch.zeros((3, height, width))\n",
        "  flops = FlopCountAnalysis(model, image)\n",
        "  flops_CT = flop_count_table(flops)\n",
        "  print(flops_CT)\n",
        "  return flops, flops_CT\n",
        "'''\n",
        "def Latency_FPS(model, height, width):\n",
        "  image = torch.rand((3, height, width))\n",
        "  iterations = 1000\n",
        "  latency = []\n",
        "  FPS = []\n",
        "\n",
        "  for i in range(iterations):\n",
        "    start = time.time()\n",
        "    output = model(image)\n",
        "    end = time.time()\n",
        "    ltc_i = end-start\n",
        "    latency.append(ltc_i)\n",
        "    FPS_i = 1/ltc_i\n",
        "    FPS.append(FPS_i)\n",
        "\n",
        "  meanLatency = mean(latency)*1000\n",
        "  stdLatency = mstd(latency)*1000\n",
        "  meanFPS = mean(FPS)*1000\n",
        "  stdFPS = mstd(latency)*1000\n",
        "  return meanLatency, stdLatency, meanFPS, stdFPS\n",
        "'''\n",
        "def fast_hist(a, b, n):\n",
        "    '''\n",
        "    a and b are predict and mask respectively\n",
        "    n is the number of classes\n",
        "    '''\n",
        "\n",
        "\n",
        "    k = (a >= 0) & (a < n)\n",
        "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
        "\n",
        "\n",
        "def per_class_iou(hist):\n",
        "    epsilon = 1e-5\n",
        "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n",
        "\n",
        "def meanIOU(num_classes, pred, target):\n",
        "  mIOU = 0\n",
        "  for i in range(len(pred)):\n",
        "      hist = fast_hist(pred[i].to(\"cpu\").numpy(), target[i].to(\"cpu\").numpy(), num_classes)\n",
        "      IOU = per_class_iou(hist)\n",
        "      mIOU = mIOU + sum(IOU)/num_classes\n",
        "  return mIOU #*100/len(pred)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp1YsgAqJpQG",
        "outputId": "944d3c71-e97c-47e3-944c-eac4618b69b2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.11.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define here your training and validation loops.\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def train(model, optimizer, train_loader, loss_fn):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "        #print(f'batch id: {batch_idx}')\n",
        "        #print(f'(inputs, targets): {(inputs.size(), targets.size())}')\n",
        "        first_image = inputs[0]\n",
        "\n",
        "        # Stampiamo le dimensioni della prima immagine nel batch\n",
        "        #print(\"Dimensioni della prima immagine nel batch:\", first_image.size())\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        outputs =  model(inputs)\n",
        "        '''\n",
        "       # print(f'outputs[0]: {outputs[0]}')\n",
        "        print(f'outputs[0] type: {outputs[0].type()}')\n",
        "        print(f'outputs[0] size: {outputs[0].size()}')\n",
        "\n",
        "\n",
        "        #print(f'targets: {targets}')\n",
        "        print(f'targets type: {targets.type()}')\n",
        "        print(f'targets size: {targets.size()}')\n",
        "        '''\n",
        "        #Ridimensioning tensor\n",
        "        targets = targets.squeeze(dim=1)\n",
        "        #print(f'targets size: {targets.size()}')\n",
        "\n",
        "        targets = targets.long()\n",
        "\n",
        "        loss = loss_fn(outputs[0], targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() # reset gradients of parameters\n",
        "        loss.backward()  # backpropagate the prediction loss\n",
        "        optimizer.step() # update model\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs[0].max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100. * correct / total\n",
        "    return train_accuracy\n",
        "\n",
        "def test(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total_images = 0\n",
        "    total_iou = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = model(inputs)\n",
        "            #Ridimensioning tensor+\n",
        "            '''\n",
        "            print(f'outputs: {outputs}')\n",
        "            print(f'outputs type: {outputs.type()}')\n",
        "            print(f'outputs size: {outputs.size()}')\n",
        "\n",
        "\n",
        "            print(f'outputs[0]: {outputs[0]}')\n",
        "\n",
        "            print(f'outputs[0] type: {outputs[0].type()}')\n",
        "            print(f'outputs[0] size: {outputs[0].size()}')\n",
        "\n",
        "\n",
        "            #pri nt(f'targets: {targets}')\n",
        "            print(f'targets type: {targets.type()}')\n",
        "            print(f'targets size: {targets.size()}')\n",
        "            '''\n",
        "            targets = targets.squeeze(dim=1)\n",
        "\n",
        "            #print(f'targets size: {targets.size()}')\n",
        "\n",
        "            targets = targets.long()\n",
        "            #print(f'targets type: {targets.type()}')\n",
        "            #print(f'targets size: {targets.size()}')\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            print(f'predicted: {predicted}')\n",
        "            iou = meanIOU(outputs.size()[1], predicted, targets) #sum of meanIOU over classes di tutte le immagini nel batch\n",
        "            #total += targets.size(0)\n",
        "            #correct += predicted.eq(targets).sum().item()\n",
        "            total_iou += iou.sum().item()  #somma di tytte le singole iou calcolate in precedenza\n",
        "\n",
        "            print(f'len di targets (=batch_size?): {len(targets)}')\n",
        "            total_images += len(targets)\n",
        "\n",
        "    result= total_iou/total_images\n",
        "    #test_loss = test_loss / len(test_loader)\n",
        "    #test_accuracy = 100. * correct / total\n",
        "    return result"
      ],
      "metadata": {
        "id": "1jqIHEeBfvoe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take dataset\n",
        "Download('drive/MyDrive/Colab Notebooks/Cityscapes.zip', '')\n",
        "Modified_CityScapes('Cityscapes/Cityspaces')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMoqWnEWfemL",
        "outputId": "46269165-0789-4f16-c05f-7f7b134681f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "The zip file has been extracted correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup fixed parameters\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "U7ozLoiIegVm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.Resize((512, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "transform_target = transforms.Compose([\n",
        "    transforms.Resize((512, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create dataloader\n",
        "dataset_train = CityScapes('Cityscapes/Cityspaces', split = 'train', transform = transform_image, label_transform = transform_target)#transform_target)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True)\n",
        "\n",
        "#print(dataset_train.__getitem__(0)[1].size())\n",
        "# Prendi il primo batch di dati dal DataLoader\n",
        "first_batch_inputs, first_batch_targets = next(iter(dataloader_train))\n",
        "\n",
        "# Calcola il numero di immagini nel primo batch\n",
        "batch_size = first_batch_inputs.size(0)\n",
        "batch_size2 = first_batch_targets.size(0)\n",
        "\n",
        "print(\"Dimensione del batch inputs nel DataLoader:\", batch_size)\n",
        "print(\"Dimensione del batch targets nel DataLoader:\", batch_size2)\n",
        "\n",
        "dataset_val = CityScapes('Cityscapes/Cityspaces', split = 'val', transform = transform_image, label_transform = transform_target)#transform_target)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "RvaKZ674agU0",
        "outputId": "18870122-e3cc-408a-f784-b2c7fac0d44d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensione del batch inputs nel DataLoader: 4\n",
            "Dimensione del batch targets nel DataLoader: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512, 256'\n",
        "#torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "lKWUuz-PiGIR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained\n",
        "pretrained = torchvision.models.resnet101(pretrained=True)\n",
        "torch.save(pretrained.state_dict(), \"DeepLab_resnet_pretrained_imagenet.pth\")\n",
        "\n",
        "# Inizialization of the model\n",
        "model = get_deeplab_v2().to(device)"
      ],
      "metadata": {
        "id": "LMZ1PAWfbewj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c930c33a-3dc7-4aef-ac0d-45db63ee63a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeplab pretraining loading...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "H3foEWjxe0_Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch number: {epoch}')\n",
        "    train(model, optimizer, dataloader_train, loss_fn)\n",
        "    test_acc = test(model, dataloader_val, loss_fn)\n",
        "    print(f\"Test accuracy: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "QRfKcxIdebta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e2b3e5ca-1e13-495a-b4d9-a24d11e97bb1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number: 0\n",
            "Test accuracy: 12216209.80\n",
            "Epoch number: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-78d124371847>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch number: {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test accuracy: {test_acc:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fe58fb505bf5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, loss_fn)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc2 = test(model, dataloader_val, loss_fn)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjjYxlV3AVo_",
        "outputId": "6174deba-06f8-4a21-a330-a92ba06ae4af"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n",
            "len di targets (=batch_size?): 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3htEb3MtORp7",
        "outputId": "c119beec-dc98-4eba-fce4-5b4876c0edbb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07684691662747435\n"
          ]
        }
      ]
    }
  ]
}