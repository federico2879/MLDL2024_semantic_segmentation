{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8476013,"sourceType":"datasetVersion","datasetId":5006503}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/federico2879/MLDL2024_semantic_segmentation.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T15:37:12.095484Z","iopub.execute_input":"2024-05-23T15:37:12.096368Z","iopub.status.idle":"2024-05-23T15:37:13.047126Z","shell.execute_reply.started":"2024-05-23T15:37:12.096328Z","shell.execute_reply":"2024-05-23T15:37:13.046222Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"fatal: destination path 'MLDL2024_semantic_segmentation' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n#from MLDL2024_semantic_segmentation.datasets.importDataset import Download\n#from MLDL2024_semantic_segmentation.datasets.importDataset import Modified_CityScapes\nfrom MLDL2024_semantic_segmentation.datasets.cityscapes import CityScapes\nfrom MLDL2024_semantic_segmentation.models.bisenet.build_bisenet import *\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:17.647691Z","iopub.execute_input":"2024-05-23T15:37:17.648472Z","iopub.status.idle":"2024-05-23T15:37:24.444016Z","shell.execute_reply.started":"2024-05-23T15:37:17.648420Z","shell.execute_reply":"2024-05-23T15:37:24.443230Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:29.244460Z","iopub.execute_input":"2024-05-23T15:37:29.245406Z","iopub.status.idle":"2024-05-23T15:37:29.302634Z","shell.execute_reply.started":"2024-05-23T15:37:29.245377Z","shell.execute_reply":"2024-05-23T15:37:29.301614Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"import gc\n\n# Function to clear GPU memory\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:32.459537Z","iopub.execute_input":"2024-05-23T15:37:32.460237Z","iopub.status.idle":"2024-05-23T15:37:32.464659Z","shell.execute_reply.started":"2024-05-23T15:37:32.460208Z","shell.execute_reply":"2024-05-23T15:37:32.463658Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\nimport statistics\n\ndef fast_hist(pred, target, num_classes):\n    k = (pred >= 0) & (pred < num_classes)\n    return np.bincount(num_classes * pred[k].astype(int) + target[k], minlength = num_classes**2).reshape(num_classes, num_classes)\n\ndef per_class_iou(hist):\n    epsilon = 1e-5\n    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n\ndef meanIOU(num_clasess, pred, target):\n  mIOU = 0\n  for i in range(len(pred)):    \n      hist = fast_hist(pred[i].cpu().numpy(), target[i].cpu().numpy(), num_classes)\n      IOU = per_class_iou(hist)\n      mIOU = mIOU + sum(IOU)/num_classes \n  return mIOU #*100/len(pred)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:34.853152Z","iopub.execute_input":"2024-05-23T15:37:34.853592Z","iopub.status.idle":"2024-05-23T15:37:34.867175Z","shell.execute_reply.started":"2024-05-23T15:37:34.853562Z","shell.execute_reply":"2024-05-23T15:37:34.866283Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"clear_memory_every = 10\n\ndef train(model, optimizer, train_loader, loss_fn):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total_iou = 0\n    total_images = 0\n\n    for batch_idx, (inputs, targets) in enumerate(train_loader):\n\n\n        #print(f'batch id: {batch_idx}')\n        #print(f'(inputs, targets): {(inputs.size(), targets.size())}')\n        first_image = inputs[0]\n\n        # Stampiamo le dimensioni della prima immagine nel batch\n        #print(\"Dimensioni della prima immagine nel batch:\", first_image.size())\n        inputs, targets = inputs.cuda(), targets.cuda()\n\n        # Compute prediction and loss\n        outputs =  model(inputs)\n        '''\n       # print(f'outputs[0]: {outputs[0]}')\n        print(f'outputs[0] type: {outputs[0].type()}')\n        print(f'outputs[0] size: {outputs[0].size()}')\n\n\n        #print(f'targets: {targets}')\n        print(f'targets type: {targets.type()}')\n        print(f'targets size: {targets.size()}')\n        '''\n        #Ridimensioning tensor\n        targets = targets.squeeze(dim=1)\n        #print(f'targets size: {targets.size()}')\n\n        targets = targets.long()\n\n        loss = loss_fn(outputs[0], targets)\n\n        # Backpropagation\n        optimizer.zero_grad() # reset gradients of parameters\n        loss.backward()  # backpropagate the prediction loss\n        optimizer.step() # update model\n\n        running_loss += loss.item()\n        _, predicted = outputs[0].max(1)\n        #print(f'predicted: {predicted}')\n        iou = meanIOU(outputs[0].size()[1], predicted, targets) #sum of meanIOU over classes di tutte le immagini nel batch\n        total_iou += iou.sum().item()  #somma di tytte le singole iou calcolate in precedenza\n        total_images += len(targets)\n        \n        # Clear GPU memory periodically\n        if batch_idx % clear_memory_every == 0:\n            clear_gpu_memory()\n\n    result= total_iou/total_images\n    return result\n\ndef test(model, test_loader, loss_fn):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total_images = 0\n    total_iou = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(test_loader):\n            inputs, targets = inputs.cuda(), targets.cuda()\n            outputs = model(inputs)\n            #Ridimensioning tensor+\n            '''\n            print(f'outputs: {outputs}')\n            print(f'outputs type: {outputs.type()}')\n            print(f'outputs size: {outputs.size()}')\n\n\n            print(f'outputs[0]: {outputs[0]}')\n\n            print(f'outputs[0] type: {outputs[0].type()}')\n            print(f'outputs[0] size: {outputs[0].size()}')\n\n\n            #pri nt(f'targets: {targets}')\n            print(f'targets type: {targets.type()}')\n            print(f'targets size: {targets.size()}')\n            '''\n            targets = targets.squeeze(dim=1)\n\n            #print(f'targets size: {targets.size()}')\n\n            targets = targets.long()\n            #print(f'targets type: {targets.type()}')\n            #print(f'targets size: {targets.size()}')\n            loss = loss_fn(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            #print(f'predicted: {predicted}')\n            iou = meanIOU(outputs.size()[1], predicted, targets) #sum of meanIOU over classes di tutte le immagini nel batch\n            #total += targets.size(0)\n            #correct += predicted.eq(targets).sum().item()\n            total_iou += iou.sum().item()  #somma di tytte le singole iou calcolate in precedenza\n\n            #print(f'len di targets (=batch_size?): {len(targets)}')\n            total_images += len(targets)\n            \n            # Clear GPU memory periodically\n            if batch_idx % clear_memory_every == 0:\n                clear_gpu_memory()\n\n    result= total_iou/total_images\n    #test_loss = test_loss / len(test_loader)\n    #test_accuracy = 100. * correct / total\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:38.494955Z","iopub.execute_input":"2024-05-23T15:37:38.495566Z","iopub.status.idle":"2024-05-23T15:37:38.510060Z","shell.execute_reply.started":"2024-05-23T15:37:38.495535Z","shell.execute_reply":"2024-05-23T15:37:38.509050Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Take dataset\n#Download('drive/MyDrive/Cityscapes.zip', '')\n#Modified_CityScapes('Cityscapes/Cityspaces')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:45.382289Z","iopub.execute_input":"2024-05-23T15:37:45.383066Z","iopub.status.idle":"2024-05-23T15:37:45.387000Z","shell.execute_reply.started":"2024-05-23T15:37:45.383034Z","shell.execute_reply":"2024-05-23T15:37:45.385943Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Setup fixed parameters\nnum_epochs = 5\nnum_classes = 19","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:52.622993Z","iopub.execute_input":"2024-05-23T15:37:52.624082Z","iopub.status.idle":"2024-05-23T15:37:52.628547Z","shell.execute_reply.started":"2024-05-23T15:37:52.624041Z","shell.execute_reply":"2024-05-23T15:37:52.627564Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Transformations\ntransform_image = transforms.Compose([\n    transforms.Resize((1024, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\ntransform_target = transforms.Compose([\n    transforms.Resize((1024, 512)),\n    transforms.ToTensor(),\n])\n\n# Create dataloader\ndataset_train = CityScapes('/kaggle/input/cityscapes/Cityscapes/Cityspaces', split = 'train', transform = transform_image, label_transform = transform_target)\ndataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n\ndataset_val = CityScapes('/kaggle/input/cityscapes/Cityscapes/Cityspaces', split = 'val', transform = transform_image, label_transform = transform_target)\ndataloader_val = DataLoader(dataset_train, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:37:58.506032Z","iopub.execute_input":"2024-05-23T15:37:58.506739Z","iopub.status.idle":"2024-05-23T15:37:58.824367Z","shell.execute_reply.started":"2024-05-23T15:37:58.506706Z","shell.execute_reply":"2024-05-23T15:37:58.823395Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Inizialization of the model\nmodel = BiSeNet(num_classes=num_classes, context_path=\"resnet18\").to(device)\nmodel = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:38:15.646204Z","iopub.execute_input":"2024-05-23T15:38:15.646616Z","iopub.status.idle":"2024-05-23T15:38:18.801735Z","shell.execute_reply.started":"2024-05-23T15:38:15.646586Z","shell.execute_reply":"2024-05-23T15:38:18.800558Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 129MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:01<00:00, 161MB/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:38:35.680370Z","iopub.execute_input":"2024-05-23T15:38:35.681168Z","iopub.status.idle":"2024-05-23T15:38:35.686685Z","shell.execute_reply.started":"2024-05-23T15:38:35.681135Z","shell.execute_reply":"2024-05-23T15:38:35.685488Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\nfor epoch in range(num_epochs):\n    train(model, optimizer, dataloader_train, loss_fn)\n    test_acc = test(model, dataloader_val, loss_fn)\n    print(f\"Test accuracy: {test_acc:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T15:38:41.200932Z","iopub.execute_input":"2024-05-23T15:38:41.201536Z","iopub.status.idle":"2024-05-23T16:18:10.741417Z","shell.execute_reply.started":"2024-05-23T15:38:41.201506Z","shell.execute_reply":"2024-05-23T16:18:10.740418Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Test accuracy: 0.08\nTest accuracy: 0.09\nTest accuracy: 0.09\nTest accuracy: 0.09\nTest accuracy: 0.09\n","output_type":"stream"}]}]}